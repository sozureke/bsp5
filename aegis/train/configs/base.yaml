# Environment settings
env:
  num_survivors: 2
  num_impostors: 1
  num_rooms: 9
  evac_room: 4
  vision_radius: 2

  # Task settings
  tasks_per_survivor: 4
  task_ticks_single: 5
  task_ticks_two_step: 3
  two_step_task_probability: 0.3

  # Kill settings
  kill_cooldown: 30

  # Door settings
  door_close_duration: 15
  door_cooldown: 40
  protect_evac_door: true

  # Meeting settings
  meeting_discussion_ticks: 60
  meeting_voting_ticks: 30

  # Win conditions
  evac_ticks_required: 5
  max_ticks: 500

  # Knower modifier (experimental)
  enable_knower: false

  # Communication
  max_messages_per_meeting: 10
  comm_mode: broadcast  # broadcast or local

  # Memory mode: none, aggregated, recurrent
  memory_mode: none

# Reward settings
rewards:
  win: 1.0
  loss: -1.0
  task_step: 0.05
  correct_report: 0.2
  impostor_ejected_survivor: 0.3
  survivor_ejected_survivor: -0.3
  kill: 0.3
  impostor_ejected_impostor: -0.4
  survivor_ejected_impostor: 0.2
  time_penalty: -0.001
  proximity_penalty: -0.01 
  proximity_penalty_ticks: 1 

# Training settings
training:
  # Algorithm
  algorithm: PPO
  
  # Number of workers
  num_workers: 1
  num_envs_per_worker: 1
  
  # Batch sizes
  train_batch_size: 1000
  sgd_minibatch_size: 64
  num_sgd_iter: 5
  
  # Learning rates
  lr: 0.0003
  entropy_coeff: 0.01
  
  # PPO specific
  clip_param: 0.2
  vf_clip_param: 10.0
  gamma: 0.99
  lambda_: 0.95
  
  # Model
  model:
    fcnet_hiddens: [256, 256]
    fcnet_activation: relu
    use_lstm: false
    lstm_cell_size: 256
    max_seq_len: 20
  
  # Training duration
  total_timesteps: 5000000
  checkpoint_freq: 50
  
  # Evaluation
  evaluation_interval: 25
  evaluation_num_episodes: 10

# Logging settings
logging:
  use_wandb: false
  wandb_project: aegis
  wandb_entity: null
  log_level: INFO
  checkpoint_dir: ./checkpoints

# Ablation toggles
ablations:
  disable_communication: false
  disable_doors: false
  random_impostor: false

